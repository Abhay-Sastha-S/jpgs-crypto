{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install unsloth vllm pillow\n\nimport sys; modules = list(sys.modules.keys())\nfor x in modules: sys.modules.pop(x) if \"PIL\" in x or \"google\" in x else None\n\nimport re\nimport torch\nfrom datasets import load_dataset, Dataset","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-26T06:36:06.152659Z","iopub.execute_input":"2025-02-26T06:36:06.152938Z","iopub.status.idle":"2025-02-26T06:37:06.956482Z","shell.execute_reply.started":"2025-02-26T06:36:06.152909Z","shell.execute_reply":"2025-02-26T06:37:06.955829Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":" %%capture\n!pip install unsloth vllm\n!pip install triton==3.1.0\n!pip install -U pynvml\nfrom unsloth import FastLanguageModel, PatchFastRL\nPatchFastRL(\"GRPO\", FastLanguageModel)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T06:52:42.405223Z","iopub.execute_input":"2025-02-26T06:52:42.405547Z","iopub.status.idle":"2025-02-26T06:54:34.546223Z","shell.execute_reply.started":"2025-02-26T06:52:42.405522Z","shell.execute_reply":"2025-02-26T06:54:34.545488Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from unsloth import is_bfloat16_supported\n\nmax_seq_length = 4084\nlora_rank = 64 \n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"Qwen/Qwen2.5-3B-Instruct\",\n    max_seq_length = max_seq_length,\n    load_in_4bit = True, \n    fast_inference = True,\n    max_lora_rank = lora_rank,\n    gpu_memory_utilization = 0.5,\n)\n\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r = lora_rank, \n    target_modules = [\n        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n        \"gate_proj\", \"up_proj\", \"down_proj\",\n    ], \n    lora_alpha = lora_rank,\n    use_gradient_checkpointing = \"unsloth\",\n    random_state = 3407,\n)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-26T06:16:09.152Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Reward function**","metadata":{}},{"cell_type":"code","source":"SYSTEM_PROMPT = \"\"\"\nRespond in the following format:\n<reasoning>\n...\n</reasoning>\n<answer>\n...\n</answer>\n\"\"\"\n\nXML_COT_FORMAT = \"\"\"\\\n<reasoning>\n{reasoning}\n</reasoning>\n<answer>\n{answer}\n</answer>\n\"\"\"\n\ndef extract_xml_answer(text: str) -> str:\n    answer = text.split(\"<answer>\")[-1]\n    answer = answer.split(\"</answer>\")[0]\n    return answer.strip()\n\ndef extract_hash_answer(text: str) -> str | None:\n    if \"####\" not in text:\n        return None\n    return text.split(\"####\")[1].strip()\n\n\ndef get_gsm8k_questions(split = \"train\") -> Dataset:\n    data = load_dataset('openai/gsm8k', 'main')[split]\n    data = data.map(lambda x: {\n        'prompt': [\n            {'role': 'system', 'content': SYSTEM_PROMPT},\n            {'role': 'user', 'content': x['question']}\n        ],\n        'answer': extract_hash_answer(x['answer'])\n    }) \n    return data \n\ndataset = get_gsm8k_questions()\n\ndef correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n    responses = [completion[0]['content'] for completion in completions]\n    q = prompts[0][-1]['content']\n    extracted_responses = [extract_xml_answer(r) for r in responses]\n    print('-'*20, f\"Question:\\n{q}\", f\"\\nAnswer:\\n{answer[0]}\", f\"\\nResponse:\\n{responses[0]}\", f\"\\nExtracted:\\n{extracted_responses[0]}\")\n    return [2.0 if r == a else 0.0 for r, a in zip(extracted_responses, answer)]\n\ndef int_reward_func(completions, **kwargs) -> list[float]:\n    responses = [completion[0]['content'] for completion in completions]\n    extracted_responses = [extract_xml_answer(r) for r in responses]\n    return [0.5 if r.isdigit() else 0.0 for r in extracted_responses]\n\ndef strict_format_reward_func(completions, **kwargs) -> list[float]:\n    #check for specific format\n    pattern = r\"^<reasoning>\\n.*?\\n</reasoning>\\n<answer>\\n.*?\\n</answer>\\n$\"\n    responses = [completion[0][\"content\"] for completion in completions]\n    matches = [re.match(pattern, r) for r in responses]\n    return [0.5 if match else 0.0 for match in matches]\n\ndef soft_format_reward_func(completions, **kwargs) -> list[float]:\n    pattern = r\"<reasoning>.*?</reasoning>\\s*<answer>.*?</answer>\"\n    responses = [completion[0][\"content\"] for completion in completions]\n    matches = [re.match(pattern, r) for r in responses]\n    return [0.5 if match else 0.0 for match in matches]\n\ndef count_xml(text) -> float:\n    count = 0.0\n    if text.count(\"<reasoning>\\n\") == 1:\n        count += 0.125\n    if text.count(\"\\n</reasoning>\\n\") == 1:\n        count += 0.125\n    if text.count(\"\\n<answer>\\n\") == 1:\n        count += 0.125\n        count -= len(text.split(\"\\n</answer>\\n\")[-1])*0.001\n    if text.count(\"\\n</answer>\") == 1:\n        count += 0.125\n        count -= (len(text.split(\"\\n</answer>\")[-1]) - 1)*0.001\n    return count\n\ndef xmlcount_reward_func(completions, **kwargs) -> list[float]:\n    contents = [completion[0][\"content\"] for completion in completions]\n    return [count_xml(c) for c in contents]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from trl import GRPOConfig, GRPOTrainer\n\ntraining_args = GRPOConfig(\n    use_vllm = True,\n    learning_rate = 5e-6,\n    adam_beta1 = 0.9,\n    adam_beta2 = 0.99,\n    weight_decay = 0.1,\n    warmup_ratio = 0.1,\n    lr_scheduler_type = \"cosine\",\n    optim = \"adamw_8bit\",\n    logging_steps = 1,\n    bf16 = is_bfloat16_supported(),\n    fp16 = not is_bfloat16_supported(),\n    per_device_train_batch_size = 1,\n    gradient_accumulation_steps = 1,\n    num_generations = 8, \n    max_prompt_length = 256,\n    max_completion_length = 200,\n    num_train_epochs = 1,\n    max_steps = 250,\n    save_steps = 250,\n    max_grad_norm = 0.1,\n    report_to = \"none\", \n    output_dir = \"outputs\",\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer = GRPOTrainer(\n    model = model,\n    processing_class = tokenizer,\n    reward_funcs = [\n        xmlcount_reward_func,\n        soft_format_reward_func,\n        strict_format_reward_func,\n        int_reward_func,\n        correctness_reward_func,\n    ],\n    args = training_args,\n    train_dataset = dataset,\n)\ntrainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# INFERENCE W/O REASONING","metadata":{}},{"cell_type":"code","source":"text = tokenizer.apply_chat_template([\n    {\"role\" : \"user\", \"content\" : \"How many r's are in strawberry?\"},\n], tokenize = False, add_generation_prompt = True)\n\nfrom vllm import SamplingParams\nsampling_params = SamplingParams(\n    temperature = 0.8,\n    top_p = 0.95,\n    max_tokens = 1024,\n)\noutput = model.fast_generate(\n    [text],\n    sampling_params = sampling_params,\n    lora_request = None,\n)[0].outputs[0].text\n\noutput","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save_lora(\"grpo_saved_lora\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# LoRA Test","metadata":{}},{"cell_type":"code","source":"text = tokenizer.apply_chat_template([\n    {\"role\" : \"system\", \"content\" : SYSTEM_PROMPT},\n    {\"role\" : \"user\", \"content\" : \"How many r's are in strawberry?\"},\n], tokenize = False, add_generation_prompt = True)\n\nfrom vllm import SamplingParams\nsampling_params = SamplingParams(\n    temperature = 0.8,\n    top_p = 0.95,\n    max_tokens = 1024,\n)\noutput = model.fast_generate(\n    text,\n    sampling_params = sampling_params,\n    lora_request = model.load_lora(\"grpo_saved_lora\"),\n)[0].outputs[0].text\n\noutput","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from vllm import SamplingParams\n\ngsm8k_dataset = get_gsm8k_questions(split=\"train\")\n\nsampling_params = SamplingParams(\n    temperature=0.0,  \n    top_p=1.0,\n    max_tokens=200,   \n)\n\ncorrect = 0\ntotal = len(gsm8k_dataset)\n\nfor example in gsm8k_dataset:\n    \n    prompt_text = tokenizer.apply_chat_template(\n        example['prompt'],\n        tokenize=False,\n        add_generation_prompt=True\n    )\n    \n    output = model.fast_generate(\n        [prompt_text],\n        sampling_params=sampling_params,\n        lora_request=None  \n    )[0].outputs[0].text\n\n    predicted_answer = extract_xml_answer(output)\n    gold_answer = example['answer']\n\n    if predicted_answer == gold_answer:\n        correct += 1\n\ngsm8k_pass_at_1 = correct / total\nprint(\"GSM8K@1PASS score:\", gsm8k_pass_at_1)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"hf_token\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Merge to 4bit\nif False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\nif False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = secret_value_0)\n\n# LoRA adapters\nif False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"lora\",)\nif False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"lora\", token = secret_value_0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}